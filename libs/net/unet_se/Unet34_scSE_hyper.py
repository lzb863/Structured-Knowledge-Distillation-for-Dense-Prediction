# coding=utf-8
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
import libs.loss.lovasz_losses as L
from libs.net.sync_batchnorm import SynchronizedBatchNorm2d


class ConvBn2d(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1, dilation=1, stride=1, groups=1, is_bn=True):
        super(ConvBn2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride,
                              dilation=dilation, groups=groups, bias=False)
        # self.bn = SynchronizedBatchNorm2d(out_channels)
        self.bn = nn.BatchNorm2d(out_channels)

    def forward(self, z):
        x = self.conv(z)
        x = self.bn(x)
        return x


class sSE(nn.Module):
    def __init__(self, out_channels):
        super(sSE, self).__init__()
        self.conv = ConvBn2d(in_channels=out_channels, out_channels=1, kernel_size=1, padding=0)

    def forward(self, x):
        x = self.conv(x)
        # print('spatial',x.size())
        x = F.sigmoid(x)
        return x


class cSE(nn.Module):
    def __init__(self, out_channels):
        super(cSE, self).__init__()
        self.conv1 = ConvBn2d(in_channels=out_channels, out_channels=int(out_channels / 2), kernel_size=1, padding=0)
        self.conv2 = ConvBn2d(in_channels=int(out_channels / 2), out_channels=out_channels, kernel_size=1, padding=0)

    def forward(self, x):
        x = nn.AvgPool2d(x.size()[2:])(x)
        # print('channel',x.size())
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.sigmoid(x)
        return x


class Decoder(nn.Module):
    def __init__(self, in_channels, channels, out_channels):
        super(Decoder, self).__init__()
        self.conv1 = ConvBn2d(in_channels, channels, kernel_size=3, padding=1)
        self.conv2 = ConvBn2d(channels, out_channels, kernel_size=3, padding=1)
        self.spatial_gate = sSE(out_channels)
        self.channel_gate = cSE(out_channels)

    def forward(self, x, e=None):
        x = F.upsample(x, scale_factor=2, mode='bilinear', align_corners=True)
        # print('x',x.size())
        # print('e',e.size())
        if e is not None:
            x = torch.cat([x, e], 1)

        x = F.relu(self.conv1(x), inplace=True)
        x = F.relu(self.conv2(x), inplace=True)
        # print('x_new',x.size())
        g1 = self.spatial_gate(x)
        # print('g1',g1.size())
        g2 = self.channel_gate(x)
        # print('g2',g2.size())
        x = g1 * x + g2 * x

        return x


class Unet_scSE_hyper(nn.Module):

    def criterion3(self, logit, truth):
        logit = logit.squeeze(1)  # [2, 1, 512, 512] 2 [2, 512, 512]
        truth = truth.squeeze(1)
        loss = L.lovasz_hinge_relu(logit, truth, per_image=True, ignore=None)
        return loss

    def criterion(self, logit, truth):
        # 2分类
        logit = logit.squeeze(1)
        truth = truth.squeeze(1)
        loss = L.lovasz_hinge(logit, truth, per_image=True, ignore=None)  # 计算损失每张图
        return loss

    def ce_loss(self, logit, truth):
        loss = self.CELoss(logit, truth)
        return loss

    def criterion_mul(self, logit, truth):
        # 多分类 , logit 输出为 BS,C,H,W  需检查C是否与cfg.MODEL_NUM_CLASSES 一致
        logit_softmax = F.softmax(logit, dim=1)
        loss = L.lovasz_softmax(logit_softmax, truth, ignore=255)
        return loss

    def criterion2(self, logit, truth):
        metric = torch.nn.BCEWithLogitsLoss(size_average=True, reduction='none')
        logit = logit.squeeze(1)
        truth = truth.squeeze(1)
        loss = metric(logit, truth)
        return loss

    def focal_loss(self, output, target, alpha, gamma, OHEM_percent):
        output = output.contiguous().view(-1)
        target = target.contiguous().view(-1).type_as(output)

        max_val = (-output).clamp(min=0)
        loss = output - output * target + max_val + ((-max_val).exp() + (-output - max_val).exp()).log()

        # This formula gives us the log sigmoid of 1-p if y is 0 and of p if y is 1
        invprobs = F.logsigmoid(-output * (target * 2 - 1))
        focal_loss = alpha * (invprobs * gamma).exp() * loss

        # Online Hard Example Mining: top x% losses (pixel-wise). Refer to http://www.robots.ox.ac.uk/~tvg/publications/2017/0026.pdf
        OHEM, _ = focal_loss.topk(k=int(OHEM_percent * [*focal_loss.shape][0]))
        return OHEM.mean()

    def __init__(self, cfg):
        super().__init__()
        self.CELoss = nn.CrossEntropyLoss(ignore_index=255, )

        self.resnet = models.resnet34(pretrained=True)

        self.conv1 = nn.Sequential(
            self.resnet.conv1,
            self.resnet.bn1,
            self.resnet.relu,
        )

        self.encoder2 = self.resnet.layer1  # 64
        self.encoder3 = self.resnet.layer2  # 128
        self.encoder4 = self.resnet.layer3  # 256
        self.encoder5 = self.resnet.layer4  # 512

        self.center = nn.Sequential(
            ConvBn2d(512, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            ConvBn2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),
        )

        self.decoder5 = Decoder(256 + 512, 512, 64)
        self.decoder4 = Decoder(64 + 256, 256, 64)
        self.decoder3 = Decoder(64 + 128, 128, 64)
        self.decoder2 = Decoder(64 + 64, 64, 64)
        self.decoder1 = Decoder(64, 32, 64)

        self.logit = nn.Sequential(
            nn.Conv2d(384, 64, kernel_size=3, padding=1),
            nn.ELU(inplace=True),
            nn.Conv2d(64, cfg.MODEL_NUM_CLASSES, kernel_size=1, padding=0),
        )

    def forward(self, x):
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        # x=torch.cat([
        #    (x-mean[2])/std[2],
        #    (x-mean[1])/std[1],
        #    (x-mean[0])/std[0],
        # ],1)
        x[:, 0, :, :] -= mean[0]
        x[:, 0, :, :] /= std[0]
        x[:, 1, :, :] -= mean[1]
        x[:, 1, :, :] /= std[1]
        x[:, 2, :, :] -= mean[2]
        x[:, 2, :, :] /= std[2]

        e1 = self.conv1(x)
        # print(e1.size())
        e2 = self.encoder2(e1)
        # print('e2',e2.size())
        e3 = self.encoder3(e2)
        # print('e3',e3.size())
        e4 = self.encoder4(e3)
        # print('e4',e4.size())
        e5 = self.encoder5(e4)
        # print('e5',e5.size())

        f = self.center(e5)
        # print('f',f.size())
        d5 = self.decoder5(f, e5)
        d4 = self.decoder4(d5, e4)
        d3 = self.decoder3(d4, e3)
        d2 = self.decoder2(d3, e2)
        d1 = self.decoder1(d2)
        # print('d1',d1.size())

        f = torch.cat((
            F.upsample(e1, scale_factor=2, mode='bilinear', align_corners=False),
            d1,
            F.upsample(d2, scale_factor=2, mode='bilinear', align_corners=False),
            F.upsample(d3, scale_factor=4, mode='bilinear', align_corners=False),
            F.upsample(d4, scale_factor=8, mode='bilinear', align_corners=False),
            F.upsample(d5, scale_factor=16, mode='bilinear', align_corners=False),
        ), 1)

        f = F.dropout2d(f, p=0.50)
        logit = self.logit(f)
        return logit

    def criterion1(self, logit, truth):
        loss = L.FocalLoss2d(gamma=0.5)(logit, truth, type='sigmoid')
        return loss

    # def criterion(self,logit, truth):
    #     loss = F.binary_cross_entropy_with_logits(logit, truth)
    #     return loss

    def metric(self, logit, truth, threshold=0.5):
        prob = F.sigmoid(logit)
        # dice = dice_accuracy(prob, truth, threshold=threshold, is_average=True)
        dice = accuracy(prob, truth, threshold=threshold, is_average=True)
        return dice

    def set_mode(self, mode, is_freeze_bn=False):
        self.mode = mode
        if mode in ['eval', 'valid', 'test']:
            self.eval()
        elif mode in ['train']:
            self.train()
            if is_freeze_bn == True:  ##freeze
                for m in self.modules():
                    if isinstance(m, nn.BatchNorm2d) or isinstance(m, SynchronizedBatchNorm2d):
                        m.eval()
                        m.weight.requires_grad = False
                        m.bias.requires_grad = False

        else:
            raise NotImplementedError


def accuracy(prob, truth, threshold=0.5, is_average=True):
    batch_size = prob.size(0)
    p = prob.detach().view(batch_size, - 1)
    t = truth.detach().view(batch_size, - 1)

    p = p > threshold
    t = t > 0.5
    correct = (p == t).float()
    accuracy = correct.sum(1) / p.size(1)

    if is_average:
        accuracy = accuracy.sum() / batch_size
        return accuracy
    else:
        return accuracy